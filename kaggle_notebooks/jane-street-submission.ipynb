{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":10140532,"sourceType":"datasetVersion","datasetId":5993004},{"sourceId":10277443,"sourceType":"datasetVersion","datasetId":6123233},{"sourceId":10334793,"sourceType":"datasetVersion","datasetId":6093233},{"sourceId":10397308,"sourceType":"datasetVersion","datasetId":6171970}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport polars as pl\nimport lightgbm as lgb\nimport pickle, random, os, logging,glob\nimport kaggle_evaluation.jane_street_inference_server\nfrom joblib import Parallel, delayed\nfrom sklearnex import patch_sklearn\nimport torch,json, math\nimport warnings\nfrom concurrent.futures import ThreadPoolExecutor\n\n\npatch_sklearn()\nlogging.getLogger('sklearnex').setLevel(logging.WARNING)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.storage\")\n\nwith open(\"/kaggle/input/feature-importance/features_information_v2.json\", mode=\"r\") as file:\n    feature_importance = json.load(file)\n\ntop_k_comb = sorted(feature_importance.items(), key=lambda x: (x[-1], x[0]), reverse=False)[:10] #best feature combination\ncols = top_k_comb[0][0].split(\"/\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-01-07T18:22:49.412629Z","iopub.execute_input":"2025-01-07T18:22:49.413035Z","iopub.status.idle":"2025-01-07T18:22:56.938363Z","shell.execute_reply.started":"2025-01-07T18:22:49.412979Z","shell.execute_reply":"2025-01-07T18:22:56.936968Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def get_slopes(n):\n    def get_slopes_power_of_2(n):\n        start = (2**(-2**-(math.log2(n)-3)))\n        ratio = start\n        return [start*ratio**i for i in range(n)]\n\n    if math.log2(n).is_integer():\n        return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n    else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n        closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround.\n        return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n\ndef get_alibi_slope(num_heads):\n    return torch.Tensor(get_slopes(num_heads)).unsqueeze(1).unsqueeze(1)\n\ndef get_alibi_bias(num_heads, seq_len):\n    relative_pos = torch.arange(seq_len)[None, :] - torch.arange(seq_len)[:, None]\n    m = get_alibi_slope(num_heads)\n    return m * relative_pos\n    \ndef init_weights(m):\n    if isinstance(m, torch.nn.Linear):\n      torch.nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out', nonlinearity='leaky_relu') #torch.nn.init.normal_(m.weight, mean=0.0, std=1/math.sqrt(6))\n      if m.bias is not None:\n        nn.init.constant_(m.bias, 0)\n    elif isinstance(m, torch.nn.Embedding):\n        torch.nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out', nonlinearity='leaky_relu') #torch.nn.init.normal_(m.weight, mean=0.0, std=1/math.sqrt(6))\n\n\nclass MultiHeadMaskAttention(torch.nn.Module):\n    def __init__(self, dim_in, dim_out, num_heads, window, bias, num_layers):\n        super().__init__()\n        self.num_heads = num_heads\n        self.pos_emb = torch.nn.Embedding(window, dim_in)\n        self.W_query = torch.nn.Linear(dim_in, dim_out*num_heads, bias=bias)\n        self.W_key = torch.nn.Linear(dim_in, dim_out*num_heads, bias=bias)\n        self.W_value = torch.nn.Linear(dim_in, dim_out*num_heads, bias=bias)\n        self.out_proj = torch.nn.Linear(dim_out, dim_out)\n        self.attn_scale = math.sqrt(dim_in)\n        self.softmax = torch.nn.Softmax(dim=-1)\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(window,window), diagonal=1).bool()\n        )\n        \n        self.register_buffer(\n            \"bias\",\n            get_alibi_bias(num_heads, window)\n        )\n\n    def forward(self, inp):\n        batch, window, emb = inp.shape\n        pos_emb = self.pos_emb(torch.arange(window, device=inp.device))\n\n        queries = self.W_query(inp)\n        keys = self.W_key(inp)\n        values = self.W_value(inp)\n\n        keys = keys.view(batch, window, self.num_heads, emb)\n        queries = queries.view(batch, window, self.num_heads, emb)\n        values = values.view(batch, window, self.num_heads, emb)\n\n        keys = keys.transpose(1,2)\n        queries = queries.transpose(1,2)\n        values = values.transpose(1,2) + pos_emb\n\n        attn_scores = (queries @ keys.mT)/self.attn_scale + self.bias\n        attn_scores = attn_scores.masked_fill(self.mask, -torch.inf)\n        attn_weights = self.softmax(attn_scores)\n        context_vectors = (attn_weights @ values).transpose(1,2).mean(dim=-2)\n        return self.out_proj(context_vectors)/0.01\n\nclass LayerNorm(torch.nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.shift = torch.nn.Parameter(torch.zeros(dim))\n        self.scale = torch.nn.Parameter(torch.ones(dim))\n\n    def forward(self, inp):\n        mean = inp.mean(dim=-1, keepdim=True)\n        var = inp.var(dim=-1, keepdim=True, unbiased=False)\n        inp_norm = (inp - mean)/(torch.sqrt(var)+self.eps)\n\n        return inp_norm + self.shift * self.scale\n\nclass FeedForwardLayer(torch.nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layer = torch.nn.Sequential(\n            torch.nn.Linear(cfg[\"n_features\"], cfg[\"n_features\"] * cfg[\"window\"]),\n            torch.nn.LeakyReLU(), #torch.nn.GELU(approximate=\"tanh\"),\n            torch.nn.Linear(cfg[\"n_features\"] * cfg[\"window\"], cfg[\"n_features\"]),\n        )\n\n\n    def forward(self, inp):\n        return self.layer(inp)\n\nclass TransformerBlock(torch.nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.attention = MultiHeadMaskAttention(dim_in=cfg[\"n_features\"], dim_out=cfg[\"n_features\"], num_heads=cfg[\"num_heads\"],\n                                                window=cfg[\"window\"], bias=cfg[\"qkv_bias\"], num_layers=cfg[\"n_layers\"])\n        self.forward_layer = FeedForwardLayer(cfg)\n        self.norm1 = LayerNorm(cfg[\"n_features\"])\n        self.norm2 = LayerNorm(cfg[\"n_features\"])\n        self.dropout = torch.nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, inp):\n        shortcut = inp\n        inp = self.norm1(inp)\n        inp = self.attention(inp)\n        inp = inp + shortcut\n\n        shortcut = inp\n        inp = self.norm2(inp)\n        inp = self.forward_layer(inp)\n        out = self.dropout(inp)\n        return out + shortcut","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T18:22:56.940906Z","iopub.execute_input":"2025-01-07T18:22:56.941494Z","iopub.status.idle":"2025-01-07T18:22:56.965112Z","shell.execute_reply.started":"2025-01-07T18:22:56.941458Z","shell.execute_reply":"2025-01-07T18:22:56.963996Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class JaneStreetModelV1(torch.nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.blocks = torch.nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n        )\n        self.norm =  LayerNorm(cfg[\"n_features\"])\n        self.final_norm = LayerNorm(cfg[\"n_features\"])\n        self.out_proj = torch.nn.Linear(cfg[\"n_features\"], 1, bias=False)\n\n    def forward(self, inp):\n        inp = self.blocks(self.norm(inp))\n        inp = inp.mean(dim=1)/0.1\n        inp = self.final_norm(inp)\n\n        return self.out_proj(inp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T18:22:56.966460Z","iopub.execute_input":"2025-01-07T18:22:56.966880Z","iopub.status.idle":"2025-01-07T18:22:56.988800Z","shell.execute_reply.started":"2025-01-07T18:22:56.966819Z","shell.execute_reply":"2025-01-07T18:22:56.987215Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class LayerNorm2(torch.nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.shift = torch.nn.Parameter(torch.zeros(dim))\n        self.scale = torch.nn.Parameter(torch.ones(dim))\n\n    def forward(self, inp):\n        mean = inp.mean(dim=-1, keepdim=True)\n        var = inp.var(dim=-1, keepdim=True, unbiased=False)\n        inp_norm = (inp - mean)/(torch.sqrt(var)+self.eps)\n\n        return inp_norm + self.shift * self.scale\n\nclass FeedForwardLayer2(torch.nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.pos_emb = torch.nn.Embedding(1, dim)\n        self.layer = torch.nn.Sequential(\n            torch.nn.Linear(dim, dim * 4),\n            torch.nn.LeakyReLU(), #torch.nn.GELU(approximate=\"tanh\"),\n            torch.nn.Linear(dim * 4, dim),\n        )\n\n    def forward(self, inp):\n        pos_emb = self.pos_emb(torch.tensor(0, device=inp.device))\n        return self.layer(inp) + pos_emb\n\nclass TransformerBlock2(torch.nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.forward_layer = FeedForwardLayer2(cfg[\"n_features\"])\n        self.norm = LayerNorm2(cfg[\"n_features\"])\n        self.dropout = torch.nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, inp):\n        shortcut = inp\n        inp = self.norm(inp)\n        inp = self.forward_layer(inp)/0.1\n        out = self.dropout(inp)\n        return out + shortcut\n\nclass JaneStreetModelV2(torch.nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.blocks = torch.nn.Sequential(\n            *[TransformerBlock2(cfg) for _ in range(cfg[\"n_layers\"])]\n        )\n        self.norm =  LayerNorm2(cfg[\"n_features\"])\n        self.final_norm = LayerNorm2(cfg[\"n_features\"])\n        self.out_proj = torch.nn.Linear(cfg[\"n_features\"], 1, bias=False)\n\n    def forward(self, inp):\n        inp = self.blocks(self.norm(inp))\n        inp = self.final_norm(inp)\n\n        return self.out_proj(inp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T18:22:56.990364Z","iopub.execute_input":"2025-01-07T18:22:56.990888Z","iopub.status.idle":"2025-01-07T18:22:57.008382Z","shell.execute_reply.started":"2025-01-07T18:22:56.990855Z","shell.execute_reply":"2025-01-07T18:22:57.007270Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_models(checkpoints, MODEL_CONFIG):\n    models = {}\n    for symbol, state in checkpoints.items():\n        model = JaneStreetModelV1(MODEL_CONFIG)\n        model.to(MODEL_CONFIG[\"device\"])\n        model.load_state_dict(state)\n        model.eval()\n        models[symbol] = model\n    return models\n\ndef load_model(checkpoint, MODEL_CONFIG):\n    model = JaneStreetModelV2(MODEL_CONFIG)\n    model.to(MODEL_CONFIG[\"device\"])\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T18:22:57.009939Z","iopub.execute_input":"2025-01-07T18:22:57.010328Z","iopub.status.idle":"2025-01-07T18:22:57.025836Z","shell.execute_reply.started":"2025-01-07T18:22:57.010293Z","shell.execute_reply":"2025-01-07T18:22:57.024623Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"MODEL_CONFIG = {\n    \"n_features\": len(cols),\n    \"window\": 4,\n    \"num_heads\": 3,\n    \"n_layers\": 3,\n    \"drop_rate\": 0.05,\n    \"qkv_bias\": False,\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n}\n\nMODEL_CONFIG2 = {\n    \"n_features\": len(cols),\n    \"n_layers\": 6,\n    \"drop_rate\": 0.1,\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T18:22:57.028437Z","iopub.execute_input":"2025-01-07T18:22:57.028843Z","iopub.status.idle":"2025-01-07T18:22:57.044695Z","shell.execute_reply.started":"2025-01-07T18:22:57.028811Z","shell.execute_reply":"2025-01-07T18:22:57.043433Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"checkpoints = torch.load(\"/kaggle/input/llm-transformer/models.pth\", weights_only=True, map_location=MODEL_CONFIG[\"device\"])\ncheckpoint2 = torch.load(\"/kaggle/input/llm-transformer/model.pth\", weights_only=True, map_location=MODEL_CONFIG2[\"device\"])\nmodels = load_models(checkpoints, MODEL_CONFIG)\nmodel2 = load_model(checkpoint2, MODEL_CONFIG2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T18:22:57.046172Z","iopub.execute_input":"2025-01-07T18:22:57.046608Z","iopub.status.idle":"2025-01-07T18:22:58.071104Z","shell.execute_reply.started":"2025-01-07T18:22:57.046555Z","shell.execute_reply":"2025-01-07T18:22:58.069825Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class JaneStreetSubmissionV1():\n    def __init__(self, cols, path_name, agg_means, nan_means, model, model_plain, \n                 window=None, history=None):\n        self.test_columns = [\"row_id\",'symbol_id', 'date_id', 'time_id']\n        self.columns = cols\n        self.fillna = nan_means\n        self.agg_means = agg_means\n        self.window = window\n        self.history = history\n        self.model = model\n        self.model_plain = model_plain\n        \n        if self.window:\n            self.df = self.get_history(path_name, symbols=list(range(39)), size=self.window*10)\n            \n\n    def get_history(self, path, symbols, size=1000):\n        parquet_file = pl.scan_parquet(path).select(\n            ['symbol_id', 'date_id', 'time_id']+self.columns\n        )\n        # Step 1: Filter and sort the data\n        df = {}\n        data = (\n            parquet_file\n            .filter(pl.col(\"symbol_id\").is_in(symbols))  # Filter for relevant symbols\n            .sort([\"date_id\", \"time_id\", \"symbol_id\"])\n            .group_by(\"symbol_id\").tail(size)\n        )\n        data = data.collect().sort([\"date_id\", \"time_id\", \"symbol_id\"])\n        for symbol in symbols:\n            df[symbol] = data.filter(pl.col(\"symbol_id\") == symbol).select(self.columns).to_numpy()\n        return df\n\n    def rolling_test(self, data):\n        size = data.shape[0] - self.window + 1\n        inputs = np.lib.stride_tricks.sliding_window_view(data, \n                                                          (self.window, data.shape[1]), \n                                                          axis=(0, 1)).reshape(size, self.window, data.shape[1])\n        return inputs\n\n    def numpy_fillna(self, arr, fillna_dict):\n        arr_copy = arr.copy()  # Avoid modifying the original array\n        for idx, col in enumerate(self.columns):\n            arr_copy[:, idx] = np.nan_to_num(arr_copy[:, idx], nan=fillna_dict[col])\n        return arr_copy\n\n    def _build(self, test_np, symbol):\n        if self.window:\n            self.df[symbol] = np.vstack((self.df[symbol][-self.history:, :], test_np))\n            test_np_filled = self.numpy_fillna(self.df[symbol], self.fillna[symbol])\n            return self.rolling_test(test_np_filled)[-1:]\n        else:\n            test_np_filled = self.numpy_fillna(test_np, self.fillna[symbol])\n            return test_np_filled\n            \n    def _forecast(self, test_np, symbol, row_ids):\n        test = self._build(test_np=test_np, symbol=symbol)\n        predictions = self.model[symbol](torch.tensor(test, device=MODEL_CONFIG[\"device\"], dtype=torch.float32)).detach().cpu().numpy() \n        return np.column_stack((row_ids, predictions))\n\n    def predict(self, test):\n        # Convert test data to numpy and filter based on symbol_id\n        test_np = test.select(self.test_columns + self.columns).to_numpy()\n        test_np_symbols = test_np[test_np[:, 1] < 39]  # Known symbols with models\n        test_np_plain = test_np[test_np[:, 1] >= 39]  # Unknown symbols without models\n    \n        # Get unique symbols for known symbols\n        unique_symbols = np.unique(test_np_symbols[:, 1])\n    \n        with torch.no_grad():\n            results = [\n                self._forecast(\n                    test_np=test_np_symbols[test_np_symbols[:, 1] == symbol][:, 4:],\n                    symbol=symbol,\n                    row_ids=test_np_symbols[test_np_symbols[:, 1] == symbol][:, 0]\n                )\n                for symbol in unique_symbols\n            ]\n    \n        # Process unknown symbols (without models)\n        if test_np_plain.size > 0: \n            row_ids = test_np_plain[:, 0]\n            test_np_plain_filled = self.numpy_fillna(test_np_plain[:, 4:], self.agg_means)\n            predictions = self.model_plain(torch.tensor(test_np_plain_filled, device=MODEL_CONFIG[\"device\"], dtype=torch.float32)).detach().cpu().numpy()\n            results.append(np.column_stack((row_ids, predictions)))\n    \n        return np.vstack(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T18:22:58.072681Z","iopub.execute_input":"2025-01-07T18:22:58.073132Z","iopub.status.idle":"2025-01-07T18:22:58.095134Z","shell.execute_reply.started":"2025-01-07T18:22:58.073084Z","shell.execute_reply":"2025-01-07T18:22:58.093923Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"path = \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9/part-0.parquet\"\nagg_means = pickle.load(open('/kaggle/input/fillnans/agg_means.p', 'rb'))\nnan_means = pickle.load(open('/kaggle/input/fillnans/nan_means.p', 'rb'))\nsubmit_model = JaneStreetSubmissionV1(cols=cols, path_name=path, agg_means=agg_means,nan_means=nan_means, \n                                      model=models, model_plain=model2, window=4, history=15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T18:22:58.097071Z","iopub.execute_input":"2025-01-07T18:22:58.097559Z","iopub.status.idle":"2025-01-07T18:23:10.668466Z","shell.execute_reply.started":"2025-01-07T18:22:58.097507Z","shell.execute_reply":"2025-01-07T18:23:10.667337Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"lags_ : pl.DataFrame | None = None\n\n# Prediction function\ndef predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pd.DataFrame:\n    \n    predictions = submit_model.predict(test)\n\n    return pd.DataFrame(predictions, columns=[\"row_id\", \"responder_6\"])","metadata":{"execution":{"iopub.status.busy":"2025-01-07T18:23:10.670018Z","iopub.execute_input":"2025-01-07T18:23:10.670461Z","iopub.status.idle":"2025-01-07T18:23:10.677014Z","shell.execute_reply.started":"2025-01-07T18:23:10.670414Z","shell.execute_reply":"2025-01-07T18:23:10.675483Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2025-01-07T18:23:10.678594Z","iopub.execute_input":"2025-01-07T18:23:10.678981Z","iopub.status.idle":"2025-01-07T18:23:11.135986Z","shell.execute_reply.started":"2025-01-07T18:23:10.678946Z","shell.execute_reply":"2025-01-07T18:23:11.134343Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}